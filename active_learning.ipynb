{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"3d7605b90d3b4b33bcb557336eb2816a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ad9953959aa54467ac930af1b43e851d","IPY_MODEL_9314576870a94c6bbe87a72be6899ce9","IPY_MODEL_ee0cb2af424a431fbc6b63611f52f05b"],"layout":"IPY_MODEL_b2f70b8df0ca49eab139ad320a67f2b2"}},"ad9953959aa54467ac930af1b43e851d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_df567681018f4b74812cfc6f4304aa15","placeholder":"‚Äã","style":"IPY_MODEL_2a56700bc8914cf7be070fa134d150e6","value":"config.json:‚Äá100%"}},"9314576870a94c6bbe87a72be6899ce9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_17d66df5b2a04087ab878d29ad278fb3","max":483,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8d4bda2598df4c7ba8ca88e80d522f34","value":483}},"ee0cb2af424a431fbc6b63611f52f05b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a1ae2eb0360a49bea0da571c4959749c","placeholder":"‚Äã","style":"IPY_MODEL_9ba4a2d6d8fe4c469694df10de5eeef8","value":"‚Äá483/483‚Äá[00:00&lt;00:00,‚Äá31.1kB/s]"}},"b2f70b8df0ca49eab139ad320a67f2b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df567681018f4b74812cfc6f4304aa15":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a56700bc8914cf7be070fa134d150e6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"17d66df5b2a04087ab878d29ad278fb3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d4bda2598df4c7ba8ca88e80d522f34":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a1ae2eb0360a49bea0da571c4959749c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ba4a2d6d8fe4c469694df10de5eeef8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e496b44937d14477bb0b3d42f8f3be2f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8081e345a3964a4fbc95681b7a920c08","IPY_MODEL_de887cf7acc84e3c9c230ff031270840","IPY_MODEL_0ef5ec536edc44df95464774cc91010a"],"layout":"IPY_MODEL_79b645f2a961477690110ba5044b8d33"}},"8081e345a3964a4fbc95681b7a920c08":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb667c3422ed43349e51080c2216632b","placeholder":"‚Äã","style":"IPY_MODEL_77a8cc3ed81b47abbfb970fae0033699","value":"model.safetensors:‚Äá100%"}},"de887cf7acc84e3c9c230ff031270840":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_662afd07b19d44d08d8b5760776147c2","max":267954768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1fa89e64c38d4df7a4b43108c674c3e0","value":267954768}},"0ef5ec536edc44df95464774cc91010a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8e47b4fd16104774b0daa622da8bf23d","placeholder":"‚Äã","style":"IPY_MODEL_5d837b1dd787441a90bebf066901c579","value":"‚Äá268M/268M‚Äá[00:05&lt;00:00,‚Äá58.6MB/s]"}},"79b645f2a961477690110ba5044b8d33":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb667c3422ed43349e51080c2216632b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"77a8cc3ed81b47abbfb970fae0033699":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"662afd07b19d44d08d8b5760776147c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1fa89e64c38d4df7a4b43108c674c3e0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8e47b4fd16104774b0daa622da8bf23d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d837b1dd787441a90bebf066901c579":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d17ce39c944f4d9c8267be67544b4eb5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5967bb32047242f18377a6fe5fecdfd8","IPY_MODEL_57a6cfc0318a4e06a14188058a0895fa","IPY_MODEL_ff36800727b54fa0b110b3cd114b1d88"],"layout":"IPY_MODEL_421e3ca644d146039ee4b3054bea5dab"}},"5967bb32047242f18377a6fe5fecdfd8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea43a10619d34de48c37f9171bd46e9e","placeholder":"‚Äã","style":"IPY_MODEL_2da1d0eec3b143eeb495890fbde4e0b1","value":"Getting‚Äápredictions:‚Äá100%"}},"57a6cfc0318a4e06a14188058a0895fa":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e828bfa3a114d2885b375573940e440","max":313,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1c59e794f21740ee8e6779f2fac5cfff","value":313}},"ff36800727b54fa0b110b3cd114b1d88":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_50dc06f3e5874d2da4c57fb0f303194f","placeholder":"‚Äã","style":"IPY_MODEL_eece3601ef5844db9d30c1d506eed3e4","value":"‚Äá313/313‚Äá[03:49&lt;00:00,‚Äá‚Äá2.24it/s]"}},"421e3ca644d146039ee4b3054bea5dab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea43a10619d34de48c37f9171bd46e9e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2da1d0eec3b143eeb495890fbde4e0b1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7e828bfa3a114d2885b375573940e440":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c59e794f21740ee8e6779f2fac5cfff":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"50dc06f3e5874d2da4c57fb0f303194f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eece3601ef5844db9d30c1d506eed3e4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["##Active Learning Pipeline for Hybrid Models\n","\n","---\n","\n"],"metadata":{"id":"Yo9h1K-uV-Z6"}},{"cell_type":"markdown","source":["##Notebook Setup and File Loading"],"metadata":{"id":"8PwA4ZzBV3Hb"}},{"cell_type":"code","source":["import pickle\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from transformers import AutoTokenizer, AutoModel\n","from peft import PeftModel, LoraConfig, get_peft_model, TaskType\n","from scipy.stats import entropy\n","from tqdm.auto import tqdm\n","import os\n","from google.colab import drive\n","\n","# Mount Google Drive to access your files\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zcaA6yV1kNZz","executionInfo":{"status":"ok","timestamp":1756581143482,"user_tz":-480,"elapsed":24829,"user":{"displayName":"joshua teo","userId":"08987881000560814542"}},"outputId":"6258f052-b04d-4965-c6c8-1eb8d3c874a6"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["## üèóÔ∏è Hybrid Model Architecture\n","\n","\n","This class defines the `HybridClassificationModel`. It's a multi-modal model that integrates three different types of features for classification: text, categorical data, and continuous data.\n","\n","\n","* **Text Branch**: Uses a DistilBERT model. LoRA (Low-Rank Adaptation) is applied to fine-tune it efficiently without modifying all the model parameters.\n","\n","* **Categorical Branch**: Each categorical feature is passed through its own embedding layer. The embeddings are then concatenated and processed by a small feed-forward network.\n","\n","* **Continuous Branch**: Continuous features are processed by a simple feed-forward network.\n","\n","* **Classifier Head**: The outputs from all three branches are concatenated and fed into a final classifier to produce the prediction."],"metadata":{"id":"3HAGJWraV8TS"}},{"cell_type":"code","source":["class HybridClassificationModel(nn.Module):\n","    def __init__(self, num_labels, categorical_feature_dim, continuous_feature_dim,\n","                 categorical_vocab_sizes=None, dropout_rate=0.1):\n","        super(HybridClassificationModel, self).__init__()\n","\n","        # Text branch (DistilBERT with LoRA)\n","        self.distilbert = AutoModel.from_pretrained('distilbert-base-uncased')\n","\n","        lora_config = LoraConfig(\n","            r=8,\n","            lora_alpha=16,\n","            target_modules=[\"q_lin\", \"k_lin\", \"v_lin\"],\n","            lora_dropout=0.05,\n","            bias=\"none\",\n","            task_type=TaskType.FEATURE_EXTRACTION\n","        )\n","        self.distilbert = get_peft_model(self.distilbert, lora_config)\n","\n","        distilbert_hidden_size = self.distilbert.config.hidden_size\n","\n","        # Dynamic categorical embeddings\n","        if categorical_vocab_sizes is None:\n","            categorical_vocab_sizes = [5, 7, 24, 10]\n","\n","        self.categorical_embeddings = nn.ModuleList([\n","            nn.Embedding(vocab_size, min(50, vocab_size // 2 + 10))\n","            for vocab_size in categorical_vocab_sizes\n","        ])\n","\n","        total_cat_embed_dim = sum(emb.embedding_dim for emb in self.categorical_embeddings)\n","\n","        self.categorical_ffn = nn.Sequential(\n","            nn.Linear(total_cat_embed_dim, distilbert_hidden_size),\n","            nn.BatchNorm1d(distilbert_hidden_size),\n","            nn.ReLU(),\n","            nn.Dropout(dropout_rate)\n","        )\n","\n","        self.continuous_ffn = nn.Sequential(\n","            nn.Linear(continuous_feature_dim, distilbert_hidden_size // 2),\n","            nn.BatchNorm1d(distilbert_hidden_size // 2),\n","            nn.ReLU(),\n","            nn.Dropout(dropout_rate),\n","            nn.Linear(distilbert_hidden_size // 2, distilbert_hidden_size),\n","            nn.BatchNorm1d(distilbert_hidden_size),\n","            nn.ReLU(),\n","            nn.Dropout(dropout_rate)\n","        )\n","\n","        combined_size = distilbert_hidden_size * 3\n","        self.classifier = nn.Sequential(\n","            nn.Linear(combined_size, distilbert_hidden_size),\n","            nn.BatchNorm1d(distilbert_hidden_size),\n","            nn.ReLU(),\n","            nn.Dropout(dropout_rate),\n","            nn.Linear(distilbert_hidden_size, distilbert_hidden_size // 2),\n","            nn.ReLU(),\n","            nn.Dropout(dropout_rate),\n","            nn.Linear(distilbert_hidden_size // 2, num_labels)\n","        )\n","\n","        self._init_weights()\n","\n","    def _init_weights(self):\n","        for module in self.modules():\n","            if isinstance(module, nn.Linear):\n","                nn.init.xavier_uniform_(module.weight)\n","                if module.bias is not None:\n","                    nn.init.zeros_(module.bias)\n","            elif isinstance(module, nn.Embedding):\n","                nn.init.xavier_uniform_(module.weight)\n","\n","    def forward(self, input_ids, attention_mask, categorical_features, continuous_features, labels=None):\n","        batch_size = input_ids.size(0)\n","\n","        # Text branch\n","        text_outputs = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n","        text_hidden = text_outputs.last_hidden_state[:, 0, :]\n","\n","        # Categorical branch\n","        cat_embeddings = []\n","        for i, embedding_layer in enumerate(self.categorical_embeddings):\n","            feature_embedded = embedding_layer(categorical_features[:, i])\n","            cat_embeddings.append(feature_embedded)\n","\n","        cat_combined = torch.cat(cat_embeddings, dim=-1)\n","        cat_processed = self.categorical_ffn(cat_combined)\n","\n","        # Continuous branch\n","        cont_processed = self.continuous_ffn(continuous_features)\n","\n","        # Combine all branches\n","        combined_features = torch.cat((text_hidden, cat_processed, cont_processed), dim=-1)\n","        logits = self.classifier(combined_features)\n","\n","        loss = None\n","        if labels is not None:\n","            loss_fct = nn.CrossEntropyLoss(label_smoothing=0.1)\n","            loss = loss_fct(logits, labels)\n","\n","        return {\"loss\": loss, \"logits\": logits}"],"metadata":{"id":"hqZQ_Da7haJm","executionInfo":{"status":"ok","timestamp":1756581143571,"user_tz":-480,"elapsed":77,"user":{"displayName":"joshua teo","userId":"08987881000560814542"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["##Hybrid Model Wrapper for Inference\n","\n","This class is designed to handle the loading of a pre-trained HybridClassificationModel and prepare it for making predictions on new data. It manages the tokenizer, model weights, and feature processing."],"metadata":{"id":"d45GGwMIWim7"}},{"cell_type":"code","source":["class HybridModelWrapper:\n","    \"\"\"Wrapper to make HybridClassificationModel work with inference\"\"\"\n","\n","    def __init__(self, model_path, config):\n","        # Load the hybrid model data\n","        hybrid_data_path = os.path.join(model_path, 'hybrid_model.bin')\n","        self.hybrid_data = torch.load(hybrid_data_path, map_location='cpu')\n","\n","        # Extract necessary information\n","        self.config = self.hybrid_data['config']\n","        self.categorical_vocab_sizes = self.hybrid_data['categorical_vocab_sizes']\n","        self.feature_metadata = self.hybrid_data['feature_metadata']\n","\n","        # Get the actual feature dimensions from the saved metadata\n","        print(\"Feature metadata keys:\", self.feature_metadata.keys())\n","\n","        if 'categorical_features' in self.feature_metadata:\n","            categorical_feature_dim = self.feature_metadata['categorical_features']['feature_dim']\n","            self.categorical_vocab_sizes = [\n","                self.feature_metadata['categorical_features']['encoders'][col]['n_classes']\n","                for col in self.feature_metadata['categorical_features']['feature_names']\n","            ]\n","        else:\n","            categorical_feature_dim = len(self.categorical_vocab_sizes)\n","\n","        if 'continuous_features' in self.feature_metadata:\n","            continuous_feature_dim = self.feature_metadata['continuous_features']['feature_dim']\n","            print(f\"Continuous feature dimension from metadata: {continuous_feature_dim}\")\n","        else:\n","            # Fallback: inspect the model state dict to get the correct dimension\n","            continuous_feature_dim = self.hybrid_data['full_model_state_dict']['continuous_ffn.0.weight'].shape[1]\n","            print(f\"Continuous feature dimension from model weights: {continuous_feature_dim}\")\n","\n","        print(f\"Creating model with categorical_dim={categorical_feature_dim}, continuous_dim={continuous_feature_dim}\")\n","\n","        # Create the hybrid model with correct dimensions\n","        self.model = HybridClassificationModel(\n","            num_labels=self.config['num_labels'],\n","            categorical_feature_dim=categorical_feature_dim,\n","            continuous_feature_dim=continuous_feature_dim,\n","            categorical_vocab_sizes=self.categorical_vocab_sizes\n","        )\n","\n","        # Get the state dict from the hybrid_data\n","        model_state_dict = self.hybrid_data['full_model_state_dict']\n","\n","        if \"class_weights\" in model_state_dict:\n","            del model_state_dict[\"class_weights\"]\n","            print(\"Successfully removed 'class_weights' from the state_dict.\")\n","\n","        # Load the cleaned state dict into the model\n","        self.model.load_state_dict(model_state_dict)\n","        self.model.eval()\n","\n","        # Store the feature dimensions for inference\n","        self.categorical_feature_dim = categorical_feature_dim\n","        self.continuous_feature_dim = continuous_feature_dim\n","\n","        # Load tokenizer\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n","\n","        # Move to GPU if available\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        self.model.to(self.device)\n","\n","        print(f\"Hybrid model loaded on device: {self.device}\")\n","\n","\n","    def extract_features_from_engineered_data(self, texts, all_features_df):\n","        \"\"\"Extract categorical and continuous features for given texts\"\"\"\n","        batch_size = len(texts)\n","\n","        # Get feature names from the model's metadata\n","        categorical_feature_names = self.feature_metadata['categorical_features']['feature_names']\n","        continuous_feature_names = self.feature_metadata['continuous_features']['feature_names']\n","\n","        # Find matching rows in the engineered features DataFrame\n","        matched_features = []\n","        for text in texts:\n","            matching_rows = all_features_df[all_features_df['text'] == text]\n","            if not matching_rows.empty:\n","                matched_features.append(matching_rows.iloc[0])\n","            else:\n","                # If no match found, create a row with default values (0 or 0.0)\n","                default_row_data = {\n","                    col: 0 for col in categorical_feature_names\n","                }\n","                default_row_data.update({\n","                    col: 0.0 for col in continuous_feature_names\n","                })\n","                matched_features.append(pd.Series(default_row_data))\n","\n","        # Convert to DataFrame for easier processing\n","        features_df = pd.DataFrame(matched_features)\n","\n","        # Extract categorical features and clip values\n","        categorical_data = []\n","        for i, col in enumerate(categorical_feature_names):\n","            if col in features_df.columns:\n","                # Get the vocabulary size for this specific feature\n","                vocab_size = self.categorical_vocab_sizes[i]\n","\n","                # Clip the values to ensure they are within the valid range\n","                values = features_df[col].astype(int).clip(0, vocab_size - 1).values\n","                categorical_data.append(values)\n","            else:\n","                categorical_data.append(np.zeros(batch_size, dtype=int))\n","\n","        categorical_features = torch.tensor(\n","            np.column_stack(categorical_data),\n","            dtype=torch.long\n","        ).to(self.device)\n","\n","        # Extract continuous features\n","        continuous_data = []\n","        for col in continuous_feature_names:\n","            if col in features_df.columns:\n","                values = pd.to_numeric(features_df[col], errors='coerce').fillna(0).astype(float).values\n","                continuous_data.append(values)\n","            else:\n","                continuous_data.append(np.zeros(batch_size, dtype=float))\n","\n","        continuous_features = torch.tensor(\n","            np.column_stack(continuous_data),\n","            dtype=torch.float32\n","        ).to(self.device)\n","\n","\n","        assert categorical_features.shape[1] == self.categorical_feature_dim, \\\n","            f\"Expected {self.categorical_feature_dim} categorical features, got {categorical_features.shape[1]}\"\n","        assert continuous_features.shape[1] == self.continuous_feature_dim, \\\n","            f\"Expected {self.continuous_feature_dim} continuous features, got {continuous_features.shape[1]}\"\n","\n","        return categorical_features, continuous_features\n","\n","    def __call__(self, texts, all_features_df, batch_size=32, return_all_scores=True):\n","        \"\"\"Pipeline-like interface for batch inference\"\"\"\n","        if isinstance(texts, str):\n","            texts = [texts]\n","\n","        all_results = []\n","\n","        # Process in batches with a progress bar\n","        for i in tqdm(range(0, len(texts), batch_size), desc=\"Getting predictions\"):\n","            batch_texts = texts[i:i + batch_size]\n","            batch_results = self._process_batch(batch_texts, all_features_df, return_all_scores)\n","            all_results.extend(batch_results)\n","\n","        return all_results\n","\n","    def _process_batch(self, texts, all_features_df, return_all_scores):\n","        \"\"\"Process a single batch\"\"\"\n","        batch_size = len(texts)\n","\n","        # Tokenize texts\n","        encoded = self.tokenizer(\n","            texts,\n","            padding=True,\n","            truncation=True,\n","            max_length=self.config.get('max_length', 512),\n","            return_tensors='pt'\n","        ).to(self.device)\n","\n","\n","        categorical_features, continuous_features = self.extract_features_from_engineered_data(texts, all_features_df)\n","\n","        with torch.no_grad():\n","            outputs = self.model(\n","                input_ids=encoded['input_ids'],\n","                attention_mask=encoded['attention_mask'],\n","                categorical_features=categorical_features,\n","                continuous_features=continuous_features\n","            )\n","\n","        # Convert to probabilities\n","        probabilities = torch.softmax(outputs['logits'], dim=-1)\n","\n","        # Format output similar to pipeline\n","        results = []\n","        for i, probs in enumerate(probabilities):\n","            if return_all_scores:\n","                result = [\n","                    {'label': f'LABEL_{j}', 'score': float(probs[j])}\n","                    for j in range(len(probs))\n","                ]\n","            else:\n","                best_idx = torch.argmax(probs)\n","                result = {'label': f'LABEL_{best_idx}', 'score': float(probs[best_idx])}\n","            results.append(result)\n","\n","        return results"],"metadata":{"id":"UBBH4K7UxFed","executionInfo":{"status":"ok","timestamp":1756581143680,"user_tz":-480,"elapsed":117,"user":{"displayName":"joshua teo","userId":"08987881000560814542"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## Active Learning Pipeline Logic"],"metadata":{"id":"pIg_cE0CXYTi"}},{"cell_type":"code","source":["def calculate_uncertainty_from_pipeline(predictions):\n","    \"\"\"Calculate entropy-based uncertainty from pipeline predictions\"\"\"\n","    uncertainties = []\n","    for pred in predictions:\n","        scores = np.array([p['score'] for p in pred])\n","        uncertainty = entropy(scores)\n","        uncertainties.append(uncertainty)\n","    return np.array(uncertainties)"],"metadata":{"id":"c_vIctcSkizO","executionInfo":{"status":"ok","timestamp":1756581143681,"user_tz":-480,"elapsed":13,"user":{"displayName":"joshua teo","userId":"08987881000560814542"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def get_predicted_labels(predictions):\n","    \"\"\"Extract predicted labels from pipeline output\"\"\"\n","    labels = []\n","    label_mapping = {\n","        'LABEL_0': 'advertisement',\n","        'LABEL_1': 'irrelevant',\n","        'LABEL_2': 'rant_without_visit',\n","        'LABEL_3': 'relevant_and_quality'\n","    }\n","\n","    for pred in predictions:\n","        best_pred = max(pred, key=lambda x: x['score'])\n","        label_name = label_mapping.get(best_pred['label'], best_pred['label'])\n","        labels.append(label_name)\n","    return np.array(labels)"],"metadata":{"id":"KTSA8x1hkkLx","executionInfo":{"status":"ok","timestamp":1756581143681,"user_tz":-480,"elapsed":5,"user":{"displayName":"joshua teo","userId":"08987881000560814542"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def class_aware_active_sampling(classifier, unlabeled_df, all_features_df, total_samples=3000):\n","    \"\"\"Select samples using class-aware uncertainty sampling\"\"\"\n","    target_samples = {\n","        'advertisement': 2500,\n","        'irrelevant': 2500,\n","        'rant_without_visit': 2500,\n","        'relevant_and_quality': 2500\n","    }\n","\n","    # Sample a subset for prediction if dataset is very large\n","    prediction_batch = unlabeled_df.sample(n=min(20000, len(unlabeled_df)))\n","    texts = prediction_batch['text'].tolist()\n","\n","    print(\"Getting model predictions...\")\n","    # Pass the all_features_df to the classifier\n","    all_predictions = classifier(texts, all_features_df=all_features_df, batch_size=64)\n","    print(f\"Example predictions: {all_predictions[:3]}\")\n","\n","\n","    print(\"Calculating uncertainties...\")\n","    uncertainty = calculate_uncertainty_from_pipeline(all_predictions)\n","    predicted_labels = get_predicted_labels(all_predictions)\n","\n","    print(f\"Predicted labels counts: {pd.Series(predicted_labels).value_counts()}\")\n","\n","    selected_indices = []\n","    print(\"Selecting samples per class...\")\n","\n","    for label, n_samples in target_samples.items():\n","        class_mask = predicted_labels == label\n","        class_df = prediction_batch[class_mask].copy()\n","\n","        if len(class_df) < n_samples:\n","            print(f\"Only {len(class_df)} samples predicted as class '{label}', taking all available.\")\n","            selected_df = class_df\n","        else:\n","            class_df['uncertainty'] = uncertainty[class_mask]\n","            selected_df = class_df.nlargest(n_samples, 'uncertainty')\n","\n","        selected_indices.extend(selected_df.index.tolist())\n","\n","    # Return user_ids instead of text_ids\n","    return unlabeled_df.loc[selected_indices]['user_id'].tolist()"],"metadata":{"id":"ZMng_ZFjkmXG","executionInfo":{"status":"ok","timestamp":1756581143682,"user_tz":-480,"elapsed":4,"user":{"displayName":"joshua teo","userId":"08987881000560814542"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["##Main Execution\n","\n","This final section brings all the pieces together. The main function loads all the necessary resources, filters out any data that has already been labeled, and then calls the class_aware_active_sampling function to select the next batch of samples for manual annotation."],"metadata":{"id":"q3Sjaop6XiVU"}},{"cell_type":"code","source":["def load_resources():\n","    CONFIG = {\n","        'base_path': '/content/drive/MyDrive/Tiktok_Hackaton/',\n","        'model_name': 'distilbert-base-uncased',\n","        'num_labels': 4,\n","        'max_length': 512\n","    }\n","\n","    # Load full engineered features from the specified Google Drive path\n","    engineered_path = os.path.join(CONFIG['base_path'], 'preprocessed_data/engineered_features.pkl')\n","    try:\n","        all_features = pd.read_pickle(engineered_path)\n","        print(f\"Successfully loaded engineered features from {engineered_path}\")\n","    except FileNotFoundError:\n","        print(f\"Error: The file {engineered_path} was not found.\")\n","        print(\"Please ensure the file exists at this location in your Google Drive.\")\n","        raise # Re-raise the exception to stop execution\n","\n","\n","    # Load existing labeled data user_ids to exclude them\n","    labeled_user_ids = set()\n","    try:\n","        file_paths = [\n","            os.path.join(CONFIG['base_path'], 'preprocessed_data/processed_train.pkl'),\n","            os.path.join(CONFIG['base_path'], 'preprocessed_data/processed_val.pkl'),\n","            os.path.join(CONFIG['base_path'], 'preprocessed_data/processed_test.pkl')\n","        ]\n","\n","        for path in file_paths:\n","            try:\n","                data_dict = pickle.load(open(path, 'rb'))\n","\n","                # Look for user_id in the data (assuming it was saved with this key)\n","                if isinstance(data_dict, dict):\n","                    # Try different possible keys for user_id\n","                    user_id_key = None\n","                    for key in ['user_id', 'review_id', 'user_ids']:\n","                        if key in data_dict:\n","                            user_id_key = key\n","                            break\n","\n","                    if user_id_key:\n","                        labeled_user_ids.update(data_dict[user_id_key])\n","                        print(f\"Loaded {len(data_dict[user_id_key])} user IDs from {os.path.basename(path)}\")\n","                    else:\n","                        print(f\"Warning: No user_id field found in {os.path.basename(path)}. Available keys: {list(data_dict.keys())}\")\n","                else:\n","                    print(f\"Warning: File {os.path.basename(path)} has unexpected format. Skipping.\")\n","\n","            except (FileNotFoundError, TypeError, KeyError) as e:\n","                print(f\"Warning loading {os.path.basename(path)}: {e}. Skipping.\")\n","\n","        if labeled_user_ids:\n","            print(f\"Total labeled user IDs found: {len(labeled_user_ids):,}\")\n","        else:\n","            print(\"No labeled data files were found or could be loaded.\")\n","\n","    except Exception as e:\n","        print(f\"An unexpected error occurred during data loading: {e}\")\n","        print(\"Assuming this is the first run and sampling from the entire dataset.\")\n","\n","\n","    # Load trained model using the hybrid wrapper\n","    TRAINED_MODEL_PATH = os.path.join(CONFIG['base_path'], 'final_model_enhanced')\n","\n","    # Define device\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    try:\n","        # Load the HybridModelWrapper\n","        classifier = HybridModelWrapper(TRAINED_MODEL_PATH, CONFIG)\n","        print(\"Successfully loaded hybrid model with custom wrapper\")\n","    except Exception as e:\n","        print(f\"Error loading hybrid model: {e}\")\n","        raise\n","\n","    return all_features, labeled_user_ids, classifier, CONFIG"],"metadata":{"id":"h0gQDnJXj_Tm","executionInfo":{"status":"ok","timestamp":1756581304165,"user_tz":-480,"elapsed":5,"user":{"displayName":"joshua teo","userId":"08987881000560814542"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["def main():\n","    print(\"Starting Active Learning Sample Selection...\")\n","\n","    all_features, labeled_user_ids, classifier, CONFIG = load_resources()\n","\n","    # Use user_id to filter out already labeled data\n","    unlabeled_pool = all_features[~all_features['user_id'].isin(labeled_user_ids)].copy()\n","\n","    print(f\"Total samples: {len(all_features):,}\")\n","    print(f\"Already labeled user IDs: {len(labeled_user_ids):,}\")\n","    print(f\"Available for selection: {len(unlabeled_pool):,}\")\n","\n","    if unlabeled_pool.empty:\n","        print(\"No unlabeled data to select. Exiting.\")\n","        return\n","\n","    selected_user_ids = class_aware_active_sampling(classifier, unlabeled_pool, all_features)\n","\n","    output_path = os.path.join(CONFIG['base_path'], 'preprocessed_data/active_learning_sample_user_ids.pkl')\n","    with open(output_path, 'wb') as f:\n","        pickle.dump(selected_user_ids, f)\n","\n","    print(f\"Selected {len(selected_user_ids)} user samples for labeling.\")\n","    print(f\"Saved list of user IDs to: {output_path}\")\n","    print(\"Ready to pass to labeling pipeline!\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"Ur8oxbHukoQ9","colab":{"base_uri":"https://localhost:8080/","height":776,"referenced_widgets":["3d7605b90d3b4b33bcb557336eb2816a","ad9953959aa54467ac930af1b43e851d","9314576870a94c6bbe87a72be6899ce9","ee0cb2af424a431fbc6b63611f52f05b","b2f70b8df0ca49eab139ad320a67f2b2","df567681018f4b74812cfc6f4304aa15","2a56700bc8914cf7be070fa134d150e6","17d66df5b2a04087ab878d29ad278fb3","8d4bda2598df4c7ba8ca88e80d522f34","a1ae2eb0360a49bea0da571c4959749c","9ba4a2d6d8fe4c469694df10de5eeef8","e496b44937d14477bb0b3d42f8f3be2f","8081e345a3964a4fbc95681b7a920c08","de887cf7acc84e3c9c230ff031270840","0ef5ec536edc44df95464774cc91010a","79b645f2a961477690110ba5044b8d33","eb667c3422ed43349e51080c2216632b","77a8cc3ed81b47abbfb970fae0033699","662afd07b19d44d08d8b5760776147c2","1fa89e64c38d4df7a4b43108c674c3e0","8e47b4fd16104774b0daa622da8bf23d","5d837b1dd787441a90bebf066901c579","d17ce39c944f4d9c8267be67544b4eb5","5967bb32047242f18377a6fe5fecdfd8","57a6cfc0318a4e06a14188058a0895fa","ff36800727b54fa0b110b3cd114b1d88","421e3ca644d146039ee4b3054bea5dab","ea43a10619d34de48c37f9171bd46e9e","2da1d0eec3b143eeb495890fbde4e0b1","7e828bfa3a114d2885b375573940e440","1c59e794f21740ee8e6779f2fac5cfff","50dc06f3e5874d2da4c57fb0f303194f","eece3601ef5844db9d30c1d506eed3e4"]},"executionInfo":{"status":"ok","timestamp":1756581571007,"user_tz":-480,"elapsed":263787,"user":{"displayName":"joshua teo","userId":"08987881000560814542"}},"outputId":"cf8d5306-8148-4453-d0bb-dc2704c5fda3"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting Active Learning Sample Selection...\n","Successfully loaded engineered features from /content/drive/MyDrive/Tiktok_Hackaton/preprocessed_data/engineered_features.pkl\n","Loaded 4382 user IDs from processed_train.pkl\n","Loaded 939 user IDs from processed_val.pkl\n","Loaded 939 user IDs from processed_test.pkl\n","Total labeled user IDs found: 6,260\n","Feature metadata keys: dict_keys(['version', 'creation_timestamp', 'pipeline_info', 'ira_analysis', 'categorical_features', 'continuous_features', 'labels', 'dataset_stats', 'processing_stats', 'text_processing', 'quality_metrics'])\n","Continuous feature dimension from metadata: 55\n","Creating model with categorical_dim=3, continuous_dim=55\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d7605b90d3b4b33bcb557336eb2816a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e496b44937d14477bb0b3d42f8f3be2f"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Successfully removed 'class_weights' from the state_dict.\n","Hybrid model loaded on device: cuda\n","Successfully loaded hybrid model with custom wrapper\n","Total samples: 50,000\n","Already labeled user IDs: 6,260\n","Available for selection: 50,000\n","Getting model predictions...\n"]},{"output_type":"display_data","data":{"text/plain":["Getting predictions:   0%|          | 0/313 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d17ce39c944f4d9c8267be67544b4eb5"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Example predictions: [[{'label': 'LABEL_0', 'score': 4.0898022325285103e-38}, {'label': 'LABEL_1', 'score': 1.0}, {'label': 'LABEL_2', 'score': 6.109869935902739e-16}, {'label': 'LABEL_3', 'score': 0.0}], [{'label': 'LABEL_0', 'score': 3.132205527388819e-14}, {'label': 'LABEL_1', 'score': 0.9999998807907104}, {'label': 'LABEL_2', 'score': 1.0968638264330366e-07}, {'label': 'LABEL_3', 'score': 4.6162542761754975e-21}], [{'label': 'LABEL_0', 'score': 4.259854665983297e-28}, {'label': 'LABEL_1', 'score': 1.0}, {'label': 'LABEL_2', 'score': 2.8021475764750114e-11}, {'label': 'LABEL_3', 'score': 3.629363022601276e-42}]]\n","Calculating uncertainties...\n","Predicted labels counts: irrelevant              15601\n","rant_without_visit       3317\n","advertisement             640\n","relevant_and_quality      442\n","Name: count, dtype: int64\n","Selecting samples per class...\n","Only 640 samples predicted as class 'advertisement', taking all available.\n","Only 442 samples predicted as class 'relevant_and_quality', taking all available.\n","Selected 6082 user samples for labeling.\n","Saved list of user IDs to: /content/drive/MyDrive/Tiktok_Hackaton/preprocessed_data/active_learning_sample_user_ids.pkl\n","Ready to pass to labeling pipeline!\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Oxy2CUddYg4e","executionInfo":{"status":"aborted","timestamp":1756581144986,"user_tz":-480,"elapsed":3,"user":{"displayName":"joshua teo","userId":"08987881000560814542"}}},"execution_count":null,"outputs":[]}]}