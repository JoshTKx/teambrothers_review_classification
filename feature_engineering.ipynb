{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPW1U0dWS8R2AUygDC/XapS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["#Notebook 1: Feature Engineering\n","\n","This notebook loads the raw review data, performs comprehensive feature\n","engineering, and saves the resulting DataFrame to an intermediate file\n","for the next stage of the pipeline."],"metadata":{"id":"zBBZGJpXcZzf"}},{"cell_type":"markdown","source":["##1. SETUP AND INSTALLATION"],"metadata":{"id":"TVCrPmfMcgj-"}},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i453nhfBPbhl","executionInfo":{"status":"ok","timestamp":1756579446547,"user_tz":-480,"elapsed":9042,"user":{"displayName":"joshua teo","userId":"08987881000560814542"}},"outputId":"5287c272-e455-41e2-cac6-1b1d6ac5a702"},"outputs":[{"output_type":"stream","name":"stdout","text":["Starting setup and installation for Notebook 1...\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","\n","Setup complete!\n","--------------------------------------------------------------------------------\n"]}],"source":["print(\"Starting setup and installation for Notebook 1...\")\n","\n","# Install all required packages\n","!pip install -q pandas scikit-learn nltk transformers torch tqdm gdown sentencepiece\n","\n","# Import libraries\n","import json\n","import pandas as pd\n","import numpy as np\n","import os\n","import re\n","import pickle\n","from pathlib import Path\n","import warnings\n","from datasets import Dataset\n","\n","# NLTK downloads\n","import nltk\n","nltk.download(\"stopwords\", quiet=True)\n","nltk.download(\"wordnet\", quiet=True)\n","nltk.download(\"punkt\", quiet=True)\n","nltk.download('punkt_tab', quiet=True)\n","\n","# Scikit-learn imports\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.decomposition import LatentDirichletAllocation\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","\n","# Transformers and Torch imports\n","import torch\n","from transformers import pipeline\n","from tqdm.auto import tqdm\n","\n","# Other utilities\n","from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Suppress warnings\n","warnings.filterwarnings('ignore')\n","\n","print(\"\\nSetup complete!\")\n","print(\"-\" * 80)"]},{"cell_type":"markdown","source":["## 2. CONFIGURATION"],"metadata":{"id":"JyRqCuuOhtXs"}},{"cell_type":"code","source":["\"\"\"## 2. CONFIGURATION\"\"\"\n","\n","print(\"Loading configuration...\")\n","\n","# Data and File Paths\n","GOOGLE_DRIVE_FILE_ID = \"1sDxExNX1y0RocYXrAE5kLjJc5cMWk0pT\"\n","INPUT_FILENAME = \"review-Wyoming_10.json\"\n","SAVE_DIR = \"/content/drive/MyDrive/Tiktok_Hackaton/preprocessed_data\"\n","OUTPUT_FILE = os.path.join(SAVE_DIR, \"engineered_features.pkl\")\n","\n","# Ensure save directory exists\n","Path(SAVE_DIR).mkdir(parents=True, exist_ok=True)\n","\n","# Data Processing Parameters\n","SAMPLE_SIZE = 50000 # Number of reviews to sample. Set to -1 to use all data.\n","CHUNK_SIZE = 32     # Batch size for sentiment analysis\n","\n","# Feature Engineering Parameters\n","NUM_TOPICS_LDA = 5\n","NUM_KEYWORDS = 5\n","SENTIMENT_MODEL_NAME = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n","\n","print(\"Configuration loaded.\")\n","print(\"-\" * 80)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s348dsPshzl3","executionInfo":{"status":"ok","timestamp":1756579446582,"user_tz":-480,"elapsed":30,"user":{"displayName":"joshua teo","userId":"08987881000560814542"}},"outputId":"e67fe879-747a-432e-9515-5bd09e2fced6"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading configuration...\n","Configuration loaded.\n","--------------------------------------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["##3. DATA COLLECTION AND INITIAL PREPROCESSING"],"metadata":{"id":"-m_twrTxcqf6"}},{"cell_type":"code","source":["\"\"\"##3. DATA COLLECTION AND INITIAL PREPROCESSING\"\"\"\n","\n","print(\"Loading and preprocessing initial data...\")\n","\n","def load_json_data(file_path):\n","    \"\"\"Loads data from a JSON file where each line is a JSON object.\"\"\"\n","    data = []\n","    try:\n","        with open(file_path, \"r\", encoding='utf-8') as f:\n","            for line_num, line in enumerate(f, 1):\n","                try:\n","                    data.append(json.loads(line))\n","                except json.JSONDecodeError as e:\n","                    print(f\"Error decoding JSON from line {line_num}: {e}\")\n","    except FileNotFoundError:\n","        print(f\"Error: The file at {file_path} was not found.\")\n","        return pd.DataFrame()\n","    except Exception as e:\n","        print(f\"Error reading file: {e}\")\n","        return pd.DataFrame()\n","\n","    df = pd.DataFrame(data)\n","    print(f\"Loaded {len(df)} records from {file_path}\")\n","    return df\n","\n","# Download the data file to the correct directory\n","input_file_path = os.path.join(SAVE_DIR, INPUT_FILENAME)\n","print(f\"Downloading {INPUT_FILENAME} to {input_file_path}...\")\n","!gdown --id {GOOGLE_DRIVE_FILE_ID} -O {input_file_path} --quiet\n","print(\"Download complete.\")\n","\n","# Load the data\n","reviews_df = load_json_data(input_file_path)\n","\n","if reviews_df.empty:\n","    raise ValueError(\"No data loaded. Please check the file and try again.\")\n","\n","print(f\"Initial dataset shape: {reviews_df.shape}\")\n","\n","# Basic data validation\n","required_columns = ['text', 'rating']\n","missing_columns = [col for col in required_columns if col not in reviews_df.columns]\n","if missing_columns:\n","    raise ValueError(f\"Missing required columns: {missing_columns}\")\n","\n","# 1. Handle missing values\n","initial_count = len(reviews_df)\n","reviews_df.dropna(subset=['text', 'rating'], inplace=True)\n","dropped_count = initial_count - len(reviews_df)\n","if dropped_count > 0:\n","    print(f\"Dropped {dropped_count} rows with missing text or rating\")\n","\n","# 2. Convert 'time' to datetime\n","if 'time' in reviews_df.columns:\n","    reviews_df['time'] = pd.to_datetime(reviews_df['time'], unit='ms', errors='coerce')\n","else:\n","    reviews_df['time'] = pd.Timestamp.now()  # Default timestamp if missing\n","\n","# 3. Basic text cleaning\n","reviews_df['cleaned_text'] = (reviews_df['text']\n","                              .astype(str)\n","                              .str.lower()\n","                              .str.strip())\n","\n","print(\"Initial data loading and preprocessing complete.\")\n","print(\"-\" * 80)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eucGNA0ocn8d","executionInfo":{"status":"ok","timestamp":1756579458717,"user_tz":-480,"elapsed":12137,"user":{"displayName":"joshua teo","userId":"08987881000560814542"}},"outputId":"f45d9992-cc01-4be0-ae30-9d100f623927"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading and preprocessing initial data...\n","Downloading review-Wyoming_10.json to /content/drive/MyDrive/Tiktok_Hackaton/preprocessed_data/review-Wyoming_10.json...\n","/usr/local/lib/python3.12/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  warnings.warn(\n","Download complete.\n","Loaded 324725 records from /content/drive/MyDrive/Tiktok_Hackaton/preprocessed_data/review-Wyoming_10.json\n","Initial dataset shape: (324725, 8)\n","Dropped 146557 rows with missing text or rating\n","Initial data loading and preprocessing complete.\n","--------------------------------------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["##4. FEATURE ENGINEERING"],"metadata":{"id":"U-vEGUXNcyf5"}},{"cell_type":"code","source":["\"\"\"##4. FEATURE ENGINEERING\"\"\"\n","\n","print(\"Starting feature engineering...\")\n","\n","# For demonstration purposes, we'll use a sample of the data if SAMPLE_SIZE is specified.\n","if SAMPLE_SIZE > 0 and len(reviews_df) > SAMPLE_SIZE:\n","    reviews_df = reviews_df.sample(n=SAMPLE_SIZE, random_state=42).reset_index(drop=True)\n","    print(f\"Using sample of {len(reviews_df)} reviews\")\n","\n","# Helper function to get the best available device\n","def get_device():\n","    if torch.cuda.is_available():\n","        device = torch.device('cuda')\n","        print(f\"Using CUDA GPU: {torch.cuda.get_device_name()}\")\n","    elif torch.backends.mps.is_available():\n","        device = torch.device('mps')\n","        print(\"Using Apple Silicon GPU (MPS)\")\n","    else:\n","        device = torch.device('cpu')\n","        print(\"Using CPU\")\n","    return device\n","\n","device = get_device()\n","print(f\"Using device: {device}\")\n","\n","print(\"Feature engineering pipeline starting...\")\n","print(\"-\" * 50)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tm_kdPyCcv3c","executionInfo":{"status":"ok","timestamp":1756579458912,"user_tz":-480,"elapsed":192,"user":{"displayName":"joshua teo","userId":"08987881000560814542"}},"outputId":"e8f5b942-c1d0-4c21-9165-3d66fc30aa0f"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting feature engineering...\n","Using sample of 50000 reviews\n","Using CUDA GPU: Tesla T4\n","Using device: cuda\n","Feature engineering pipeline starting...\n","--------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["###  [1/9] Sentiment Analysis"],"metadata":{"id":"7dI004KTc8-L"}},{"cell_type":"code","source":["\"\"\"### [1/9] Sentiment Analysis\"\"\"\n","\n","print(\"[1/9] Performing Sentiment Analysis...\")\n","\n","try:\n","    sentiment_pipeline = pipeline(\n","        \"sentiment-analysis\",\n","        model=SENTIMENT_MODEL_NAME,\n","        device=0 if device.type == 'cuda' else -1,\n","        padding=True,\n","        truncation=True,\n","        max_length=512\n","    )\n","\n","    # Corrected line: Pass the list of texts directly to the pipeline\n","    texts_to_analyze = reviews_df['cleaned_text'].tolist()\n","    batch_results = sentiment_pipeline(texts_to_analyze, batch_size=CHUNK_SIZE)\n","\n","    # Process results\n","    sent_labels = [res['label'] for res in batch_results]\n","    sent_scores = [res['score'] for res in batch_results]\n","\n","    # Updated score mapping for the twitter-roberta model\n","    score_map = {'LABEL_0': -1, 'LABEL_1': 0, 'LABEL_2': 1, 'NEGATIVE': -1, 'NEUTRAL': 0, 'POSITIVE': 1}\n","    reviews_df['sentiment_score'] = [\n","        score_map.get(label, 0) * score for label, score in zip(sent_labels, sent_scores)\n","    ]\n","\n","    print(\"Sentiment analysis complete.\")\n","\n","except Exception as e:\n","    print(f\"Error in sentiment analysis: {e}\")\n","    reviews_df['sentiment_score'] = 0.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wkv5hfQ8c3i7","outputId":"779ef0c9-86e8-4c95-ae04-c7eba5d7b3af"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[1/9] Performing Sentiment Analysis...\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Device set to use cuda:0\n"]}]},{"cell_type":"markdown","source":["### [2/9] Time-based features"],"metadata":{"id":"KV8dR91gdMFS"}},{"cell_type":"code","source":["\"\"\"### [2/9] Time-based features\"\"\"\n","\n","print(\"[2/9] Extracting Time-based Features...\")\n","\n","# Ensure time column is properly formatted\n","if 'time' not in reviews_df.columns or reviews_df['time'].isna().all():\n","    reviews_df['time'] = pd.Timestamp.now()\n","    print(\"Warning: Using default timestamp for missing time data\")\n","\n","reviews_df[\"review_day_of_week\"] = reviews_df[\"time\"].dt.dayofweek\n","reviews_df[\"review_hour_of_day\"] = reviews_df[\"time\"].dt.hour\n","\n","print(\"Time-based features complete.\")"],"metadata":{"id":"O7-adouRdLAU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###[3/9] Topic Modeling (LDA)"],"metadata":{"id":"4I5pIeoBdX0_"}},{"cell_type":"code","source":["\"\"\"### [3/9] Topic Modeling (LDA)\"\"\"\n","\n","print(\"[3/9] Performing Topic Modeling...\")\n","\n","stop_words = set(stopwords.words(\"english\"))\n","lemmatizer = WordNetLemmatizer()\n","\n","def preprocess_text_for_topic_modeling(text):\n","    \"\"\"Preprocess text for topic modeling\"\"\"\n","    try:\n","        tokens = word_tokenize(str(text))\n","        tokens = [lemmatizer.lemmatize(word.lower()) for word in tokens\n","                 if word.isalpha() and word.lower() not in stop_words and len(word) > 2]\n","        return \" \".join(tokens)\n","    except Exception as e:\n","        return \"\"\n","\n","reviews_df[\"processed_text_for_topic\"] = reviews_df[\"cleaned_text\"].apply(preprocess_text_for_topic_modeling)\n","\n","# Remove empty processed texts\n","non_empty_mask = reviews_df[\"processed_text_for_topic\"].str.len() > 0\n","processed_texts = reviews_df.loc[non_empty_mask, \"processed_text_for_topic\"]\n","\n","if len(processed_texts) > NUM_TOPICS_LDA and len(processed_texts) > 10:\n","    try:\n","        vectorizer = TfidfVectorizer(\n","            max_df=0.95,\n","            min_df=2,\n","            stop_words=\"english\",\n","            max_features=1000\n","        )\n","        dtm = vectorizer.fit_transform(processed_texts)\n","\n","        lda = LatentDirichletAllocation(\n","            n_components=NUM_TOPICS_LDA,\n","            random_state=42,\n","            max_iter=10  # Reduce iterations for faster processing\n","        )\n","        lda.fit(dtm)\n","\n","        # Get topic assignments for all texts\n","        all_dtm = vectorizer.transform(reviews_df[\"processed_text_for_topic\"])\n","        reviews_df[\"dominant_topic\"] = lda.transform(all_dtm).argmax(axis=1)\n","\n","        print(f\"Topic modeling complete with {NUM_TOPICS_LDA} topics.\")\n","\n","    except Exception as e:\n","        print(f\"Error in topic modeling: {e}\")\n","        reviews_df[\"dominant_topic\"] = 0\n","else:\n","    reviews_df[\"dominant_topic\"] = 0\n","    print(\"Insufficient data for topic modeling, using default topic.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5yapkrIQdTpL","executionInfo":{"status":"ok","timestamp":1756579961412,"user_tz":-480,"elapsed":10289,"user":{"displayName":"joshua teo","userId":"08987881000560814542"}},"outputId":"d872f56c-1d38-4dd6-9175-2996d02b0f77"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["[3/9] Performing Topic Modeling...\n","Topic modeling complete with 5 topics.\n"]}]},{"cell_type":"markdown","source":["### [4/9] Keyword Extraction"],"metadata":{"id":"OlJ4wu5GeH1t"}},{"cell_type":"code","source":["\"\"\"### [4/9] Keyword Extraction\"\"\"\n","\n","print(\"[4/9] Performing Keyword Extraction...\")\n","\n","if len(reviews_df) > NUM_KEYWORDS:\n","    try:\n","        vectorizer_keywords = TfidfVectorizer(\n","            stop_words=\"english\",\n","            max_features=1000,\n","            min_df=2\n","        )\n","\n","        texts_for_keywords = reviews_df[\"processed_text_for_topic\"].fillna(\"\")\n","        non_empty_texts = texts_for_keywords[texts_for_keywords.str.len() > 0]\n","\n","        if len(non_empty_texts) > 0:\n","            tfidf_matrix = vectorizer_keywords.fit_transform(non_empty_texts)\n","            feature_names = vectorizer_keywords.get_feature_names_out()\n","\n","            def get_top_n_keywords(row, feature_names, n=NUM_KEYWORDS):\n","                \"\"\"Extract top n keywords from TF-IDF row\"\"\"\n","                if np.sum(row) == 0:\n","                    return ['no_keywords'] * n\n","                top_n_indices = row.argsort()[-n:][::-1]\n","                return [feature_names[i] for i in top_n_indices]\n","\n","            # Get keywords for non-empty texts\n","            keywords_sparse = vectorizer_keywords.transform(texts_for_keywords)\n","            reviews_df[\"top_keywords\"] = [\n","                get_top_n_keywords(row.toarray().flatten(), feature_names)\n","                for row in keywords_sparse\n","            ]\n","\n","            print(\"Keyword extraction complete.\")\n","        else:\n","            reviews_df[\"top_keywords\"] = [['no_keywords'] * NUM_KEYWORDS] * len(reviews_df)\n","            print(\"No valid text for keyword extraction.\")\n","\n","    except Exception as e:\n","        print(f\"Error in keyword extraction: {e}\")\n","        reviews_df[\"top_keywords\"] = [['no_keywords'] * NUM_KEYWORDS] * len(reviews_df)\n","else:\n","    reviews_df[\"top_keywords\"] = [['no_keywords'] * NUM_KEYWORDS] * len(reviews_df)\n","    print(\"Insufficient data for keyword extraction.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y2DOu1sLd7Yv","executionInfo":{"status":"ok","timestamp":1756579968522,"user_tz":-480,"elapsed":7097,"user":{"displayName":"joshua teo","userId":"08987881000560814542"}},"outputId":"6df44b88-f5af-4d08-fd85-ee054ea60e8c"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["[4/9] Performing Keyword Extraction...\n","Keyword extraction complete.\n"]}]},{"cell_type":"markdown","source":["###[5/9] Policy Insight Features"],"metadata":{"id":"-dVba1g5eNa8"}},{"cell_type":"code","source":["\"\"\"### [5/9] Policy Insight Features\"\"\"\n","\n","print(\"[5/9] Adding Policy Insight Features...\")\n","\n","# Ensure text column is string type\n","reviews_df[\"cleaned_text\"] = reviews_df[\"cleaned_text\"].fillna(\"\").astype(str)\n","\n","# URL detection\n","URL_PATTERNS = [\n","    r\"(?:https?://|www\\.)\",  # Standard URLs\n","    r\"\\b\\w+\\.\\w{2,4}(?:/\\S*)?\\b\"  # Domain-like patterns\n","]\n","reviews_df[\"num_urls\"] = sum(\n","    reviews_df[\"cleaned_text\"].str.count(pattern, flags=re.I)\n","    for pattern in URL_PATTERNS\n",")\n","\n","# Email detection\n","EMAIL_PAT = r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\"\n","reviews_df[\"num_emails\"] = reviews_df[\"cleaned_text\"].str.count(EMAIL_PAT, flags=re.I)\n","\n","# Phone number detection (improved pattern)\n","PHONE_PATTERNS = [\n","    r\"\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b\",  # US format\n","    r\"\\b\\(\\d{3}\\)\\s?\\d{3}[-.]?\\d{4}\\b\",  # (123) 456-7890\n","    r\"\\b\\d{10,15}\\b\"  # General long numbers\n","]\n","reviews_df[\"num_phone_numbers\"] = sum(\n","    reviews_df[\"cleaned_text\"].str.count(pattern)\n","    for pattern in PHONE_PATTERNS\n",")\n","\n","# Social media patterns\n","MENTION_PAT = r\"@\\w+\"\n","reviews_df[\"num_mentions\"] = reviews_df[\"cleaned_text\"].str.count(MENTION_PAT)\n","\n","HASHTAG_PAT = r\"#\\w+\"\n","reviews_df[\"num_hashtags\"] = reviews_df[\"cleaned_text\"].str.count(HASHTAG_PAT)\n","\n","# Irrelevant content indicators\n","irrelevant_words = [\n","    \"iphone\", \"crypto\", \"bitcoin\", \"forex\", \"makeup\", \"subscribe\",\n","    \"channel\", \"follow\", \"like\", \"comment\", \"share\", \"promo\", \"discount\"\n","]\n","IRRELEVANT_PAT = r\"\\b(?:\" + \"|\".join(map(re.escape, irrelevant_words)) + r\")\\b\"\n","reviews_df[\"num_irrelevant_words\"] = reviews_df[\"cleaned_text\"].str.count(IRRELEVANT_PAT, flags=re.I)\n","\n","print(\"Policy insight features complete.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oiVuZFtgeLTd","executionInfo":{"status":"ok","timestamp":1756579968723,"user_tz":-480,"elapsed":195,"user":{"displayName":"joshua teo","userId":"08987881000560814542"}},"outputId":"18978932-60a4-4e62-9210-13946f1f86ff"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["[5/9] Adding Policy Insight Features...\n","Policy insight features complete.\n"]}]},{"cell_type":"markdown","source":["### [6/9] Stylistic Features"],"metadata":{"id":"b4PpsqqheWTB"}},{"cell_type":"code","source":["\"\"\"### [6/9] Stylistic Features\"\"\"\n","\n","print(\"[6/9] Extracting Stylistic Features...\")\n","\n","texts = reviews_df[\"cleaned_text\"].fillna(\"\")\n","\n","# Punctuation and style markers\n","reviews_df[\"num_exclamations\"] = texts.str.count(\"!\")\n","reviews_df[\"num_questions\"] = texts.str.count(r\"\\?\")\n","reviews_df[\"num_ellipsis\"] = texts.str.count(r\"\\.{3,}\")\n","\n","# Total punctuation count\n","all_punctuation = \"!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\n","reviews_df[\"num_total_punctuation\"] = texts.apply(\n","    lambda s: sum(1 for char in s if char in all_punctuation)\n",")\n","\n","# Text length and word statistics\n","reviews_df[\"review_length_words\"] = texts.str.split().apply(len)\n","reviews_df[\"review_length_chars\"] = texts.str.len()\n","\n","# Capitalization patterns\n","reviews_df[\"all_caps_word_count\"] = texts.str.findall(r\"\\b[A-Z]{2,}\\b\").apply(len)\n","reviews_df[\"elongated_word_count\"] = texts.str.count(r\"\\b\\w*(\\w)\\1{2,}\\w*\\b\")\n","\n","# Ratios for normalization\n","def safe_ratio(numerator, denominator, default=0.0):\n","    \"\"\"Calculate ratio with safe division\"\"\"\n","    return numerator / max(1, denominator) if denominator > 0 else default\n","\n","reviews_df[\"caps_ratio\"] = texts.apply(\n","    lambda t: safe_ratio(sum(ch.isupper() for ch in t), sum(ch.isalpha() for ch in t))\n",")\n","\n","reviews_df[\"unique_word_ratio\"] = texts.apply(\n","    lambda t: safe_ratio(len(set(t.split())), len(t.split()))\n",")\n","\n","reviews_df[\"digit_ratio\"] = texts.apply(\n","    lambda t: safe_ratio(sum(ch.isdigit() for ch in t), len(t))\n",")\n","\n","reviews_df[\"punctuation_ratio\"] = texts.apply(\n","    lambda t: safe_ratio(\n","        sum(1 for ch in t if not ch.isalnum() and not ch.isspace()),\n","        len(t)\n","    )\n",")\n","\n","print(\"Stylistic features complete.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gEOLjL33eUhy","executionInfo":{"status":"ok","timestamp":1756579971429,"user_tz":-480,"elapsed":2635,"user":{"displayName":"joshua teo","userId":"08987881000560814542"}},"outputId":"d5eb318a-cdaf-4876-ecd1-7c1ea076ab5d"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["[6/9] Extracting Stylistic Features...\n","Stylistic features complete.\n"]}]},{"cell_type":"markdown","source":["###[7/9] Emotional & Expressive Marker Extractor"],"metadata":{"id":"m10OUYGuejuy"}},{"cell_type":"code","source":["\"\"\"### [7/9] Emotional & Expressive Marker Extractor\"\"\"\n","\n","print(\"[7/9] Extracting Emotional Markers...\")\n","\n","texts = reviews_df[\"cleaned_text\"].fillna(\"\")\n","\n","# Sentiment word lists (expanded)\n","POS_TERMS = [\n","    \"love\", \"amazing\", \"awesome\", \"fantastic\", \"great\", \"good\", \"wonderful\",\n","    \"excellent\", \"happy\", \"enjoy\", \"beautiful\", \"nice\", \"friendly\", \"delicious\",\n","    \"tasty\", \"perfect\", \"outstanding\", \"brilliant\", \"superb\", \"marvelous\"\n","]\n","\n","NEG_TERMS = [\n","    \"bad\", \"worse\", \"worst\", \"awful\", \"terrible\", \"horrible\", \"disgusting\",\n","    \"hate\", \"scam\", \"rude\", \"dirty\", \"cold\", \"bland\", \"overpriced\", \"poor\",\n","    \"slow\", \"angry\", \"sad\", \"disappoint\", \"nightmare\", \"disaster\", \"pathetic\"\n","]\n","\n","# Count sentiment words\n","POS_RE = r\"\\b(?:\" + \"|\".join(map(re.escape, POS_TERMS)) + r\")\\b\"\n","NEG_RE = r\"\\b(?:\" + \"|\".join(map(re.escape, NEG_TERMS)) + r\")\\b\"\n","\n","reviews_df[\"num_sentiment_words_pos\"] = texts.str.count(POS_RE, flags=re.IGNORECASE)\n","reviews_df[\"num_sentiment_words_neg\"] = texts.str.count(NEG_RE, flags=re.IGNORECASE)\n","\n","# Emoji detection (simplified pattern)\n","EMOJI_PATTERN = r\"[😀-🙏🚀-🛿☀-➿]\"\n","reviews_df[\"num_emojis\"] = texts.str.count(EMOJI_PATTERN)\n","\n","# Derived features\n","reviews_df[\"sentiment_polarity_lex\"] = (\n","    reviews_df[\"num_sentiment_words_pos\"] - reviews_df[\"num_sentiment_words_neg\"]\n",")\n","\n","# Per 100 words normalization\n","word_counts = texts.str.split().apply(len).replace(0, 1)\n","reviews_df[\"pos_words_per_100w\"] = reviews_df[\"num_sentiment_words_pos\"] * 100 / word_counts\n","reviews_df[\"neg_words_per_100w\"] = reviews_df[\"num_sentiment_words_neg\"] * 100 / word_counts\n","reviews_df[\"emojis_per_100w\"] = reviews_df[\"num_emojis\"] * 100 / word_counts\n","\n","print(\"Emotional markers complete.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fczCI2x-eaEm","executionInfo":{"status":"ok","timestamp":1756579974740,"user_tz":-480,"elapsed":3307,"user":{"displayName":"joshua teo","userId":"08987881000560814542"}},"outputId":"0551ec8d-e07f-4924-bace-d6e1602acc09"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["[7/9] Extracting Emotional Markers...\n","Emotional markers complete.\n"]}]},{"cell_type":"markdown","source":["### [8/9] User Information Extractor"],"metadata":{"id":"yoA0SplleqX4"}},{"cell_type":"code","source":["\"\"\"### [8/9] User Information Extractor\"\"\"\n","\n","print(\"[8/9] Adding User-level Features...\")\n","\n","# Ensure user_id exists and is properly formatted\n","if 'user_id' not in reviews_df.columns:\n","    reviews_df['user_id'] = 'anonymous_' + reviews_df.index.astype(str)\n","    print(\"Warning: No user_id found, created anonymous IDs\")\n","\n","reviews_df[\"user_id\"] = reviews_df[\"user_id\"].fillna(\"unknown\").astype(str)\n","\n","# Basic user statistics\n","reviews_df[\"user_review_count\"] = reviews_df.groupby(\"user_id\")[\"user_id\"].transform(\"size\")\n","\n","# Same day reviews (requires valid time column)\n","if reviews_df['time'].notna().any():\n","    reviews_df[\"num_same_day_reviews\"] = reviews_df.groupby(\n","        [\"user_id\", reviews_df[\"time\"].dt.floor(\"D\")]\n","    )[\"user_id\"].transform(\"size\")\n","else:\n","    reviews_df[\"num_same_day_reviews\"] = 1\n","\n","# Rating statistics\n","if 'rating' in reviews_df.columns:\n","    reviews_df[\"avg_review_rating\"] = reviews_df.groupby(\"user_id\")[\"rating\"].transform(\"mean\")\n","else:\n","    reviews_df[\"avg_review_rating\"] = 5.0\n","\n","# Burst analysis (7-day rolling window)\n","try:\n","    if reviews_df['time'].notna().sum() > 0:\n","        per_user_burst = (reviews_df.assign(day=reviews_df[\"time\"].dt.floor(\"D\"))\n","                         .groupby(\"user_id\")\n","                         .apply(lambda g: (g.set_index(\"day\")\n","                                          .resample(\"D\")[\"user_id\"]\n","                                          .size()\n","                                          .rolling(7, min_periods=1)\n","                                          .sum()\n","                                          .max()))\n","                         .rename(\"user_max_7d_burst\")\n","                         .reset_index())\n","        reviews_df = reviews_df.merge(per_user_burst, on=\"user_id\", how=\"left\")\n","        reviews_df[\"user_max_7d_burst\"] = reviews_df[\"user_max_7d_burst\"].fillna(1)\n","    else:\n","        reviews_df[\"user_max_7d_burst\"] = 1\n","except Exception as e:\n","    print(f\"Error in burst analysis: {e}\")\n","    reviews_df[\"user_max_7d_burst\"] = 1\n","\n","# Time span analysis\n","if reviews_df['time'].notna().sum() > 0:\n","    reviews_df[\"user_review_span_days\"] = reviews_df.groupby(\"user_id\")[\"time\"].transform(\n","        lambda x: (x.max() - x.min()).days + 1 if x.notna().any() else 1\n","    )\n","else:\n","    reviews_df[\"user_review_span_days\"] = 1\n","\n","# Derived ratios\n","reviews_df[\"user_burst_ratio\"] = (\n","    reviews_df[\"user_max_7d_burst\"].astype(float) /\n","    reviews_df[\"user_review_count\"].replace(0, 1)\n",")\n","\n","# Place diversity (requires gmap_id)\n","if 'gmap_id' in reviews_df.columns:\n","    reviews_df[\"user_place_diversity\"] = reviews_df.groupby(\"user_id\")[\"gmap_id\"].transform(\"nunique\")\n","else:\n","    reviews_df[\"user_place_diversity\"] = 1\n","\n","# Text statistics\n","reviews_df[\"user_avg_length\"] = reviews_df.groupby(\"user_id\")[\"text\"].transform(\n","    lambda s: s.astype(str).str.len().mean()\n",")\n","\n","reviews_df[\"user_text_share\"] = (\n","    (reviews_df[\"text\"].astype(str).str.strip().ne(\"\") & reviews_df[\"text\"].notna())\n","    .groupby(reviews_df[\"user_id\"])\n","    .transform(\"mean\")\n","    .fillna(0.0)\n",")\n","\n","# Placeholder for response rate (would need actual response data)\n","reviews_df[\"user_response_rate\"] = 0.0\n","\n","print(\"User-level features complete.\")"],"metadata":{"id":"Zv3eU5yPeo-C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###[9/9] Photo Extractor"],"metadata":{"id":"7mjlIRn5ezcz"}},{"cell_type":"code","source":["\"\"\"### [9/9] Photo Extractor\"\"\"\n","\n","print(\"[9/9] Adding Photo-based Features...\")\n","\n","def extract_pic_urls(pics):\n","    \"\"\"Extract photo URLs from various formats\"\"\"\n","    if pics is None or (isinstance(pics, float) and pd.isna(pics)):\n","        return []\n","\n","    if isinstance(pics, str):\n","        try:\n","            return extract_pic_urls(json.loads(pics))\n","        except:\n","            return [pics] if \"http\" in pics else []\n","\n","    if isinstance(pics, list):\n","        urls = []\n","        for item in pics:\n","            urls.extend(extract_pic_urls(item))\n","        return urls\n","\n","    if isinstance(pics, dict):\n","        return extract_pic_urls(pics.get(\"url\", []))\n","\n","    return []\n","\n","# Handle photos column\n","if 'pics' in reviews_df.columns:\n","    urls_series = reviews_df[\"pics\"].apply(extract_pic_urls)\n","else:\n","    print(\"Warning: No 'pics' column found, using empty photo data\")\n","    urls_series = pd.Series([[]] * len(reviews_df))\n","\n","# Basic photo features\n","reviews_df[\"n_photos\"] = urls_series.apply(len).astype(int)\n","\n","# URL length statistics\n","reviews_df[\"photo_url_length_mean\"] = urls_series.apply(\n","    lambda xs: float(np.mean([len(str(u)) for u in xs]) if xs else 0.0)\n",")\n","\n","reviews_df[\"photo_url_length_max\"] = urls_series.apply(\n","    lambda xs: float(max([len(str(u)) for u in xs]) if xs else 0.0)\n",")\n","\n","reviews_df[\"photo_url_length_std\"] = urls_series.apply(\n","    lambda xs: float(np.std([len(str(u)) for u in xs]) if len(xs) > 1 else 0.0)\n",")\n","\n","# CDN detection\n","reviews_df[\"photo_google_cdn_flag\"] = urls_series.apply(\n","    lambda xs: int(any(\"googleusercontent\" in str(u) for u in xs))\n",")\n","\n","# Photo-text relationship features\n","reviews_df[\"has_photo_and_no_text\"] = (\n","    (reviews_df[\"n_photos\"] > 0) &\n","    (reviews_df[\"text\"].fillna(\"\").str.strip() == \"\")\n",").astype(int)\n","\n","reviews_df[\"has_photo_and_short_text\"] = (\n","    (reviews_df[\"n_photos\"] > 0) &\n","    (reviews_df[\"text\"].fillna(\"\").str.len() < 20)\n",").astype(int)\n","\n","# Photo-rating relationships\n","if 'rating' in reviews_df.columns:\n","    reviews_df[\"has_photo_and_low_rating\"] = (\n","        (reviews_df[\"n_photos\"] > 0) &\n","        (reviews_df[\"rating\"].fillna(0) <= 2)\n","    ).astype(int)\n","\n","    reviews_df[\"has_photo_and_high_rating\"] = (\n","        (reviews_df[\"n_photos\"] > 0) &\n","        (reviews_df[\"rating\"].fillna(0) >= 4)\n","    ).astype(int)\n","else:\n","    reviews_df[\"has_photo_and_low_rating\"] = 0\n","    reviews_df[\"has_photo_and_high_rating\"] = 0\n","\n","# User-level photo statistics (z-score normalization)\n","user_photo_stats = reviews_df.groupby(\"user_id\")[\"n_photos\"].agg(['mean', 'std']).fillna(0)\n","reviews_df = reviews_df.merge(\n","    user_photo_stats,\n","    left_on=\"user_id\",\n","    right_index=True,\n","    suffixes=('', '_user')\n",")\n","\n","reviews_df[\"n_photos_user_z\"] = (\n","    (reviews_df[\"n_photos\"] - reviews_df[\"mean\"]) /\n","    reviews_df[\"std\"].replace(0, 1)\n",").fillna(0.0)\n","\n","# Initialize remaining photo features that are expected by the second notebook\n","photo_features_to_initialize = [\n","    \"photo_url_param_count_mean\", \"photo_host_nunique\", \"photo_has_multi_hosts\",\n","    \"photo_has_thumbnail_param\", \"photo_url_is_duplicate_any\",\n","    \"user_photo_host_diversity\", \"user_avg_photo_url_len\", \"user_median_n_photos\",\n","    \"place_photo_host_diversity\", \"place_median_n_photos\"\n","]\n","\n","for col in photo_features_to_initialize:\n","    if col not in reviews_df.columns:\n","        reviews_df[col] = 0.0\n","\n","# Clean up temporary columns\n","reviews_df = reviews_df.drop(['mean', 'std'], axis=1, errors='ignore')\n","\n","print(\"Photo-based features complete.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VHZsX6BIewJX","executionInfo":{"status":"ok","timestamp":1756580009061,"user_tz":-480,"elapsed":10,"user":{"displayName":"joshua teo","userId":"08987881000560814542"}},"outputId":"3259510f-8abf-4990-e5c6-612d3d75f9e8"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["[9/9] Adding Photo-based Features...\n","Photo-based features complete.\n"]}]},{"cell_type":"markdown","source":["##5. DATA VALIDATION AND SAVE"],"metadata":{"id":"SpqdCtJEfD99"}},{"cell_type":"code","source":["\"\"\"## 5. DATA VALIDATION AND SAVE\"\"\"\n","\n","print(\"Validating and saving engineered features...\")\n","\n","# Data validation\n","print(\"Performing data validation...\")\n","\n","# Check for infinite or extremely large values\n","numeric_columns = reviews_df.select_dtypes(include=[np.number]).columns\n","for col in numeric_columns:\n","    if np.isinf(reviews_df[col]).any():\n","        print(f\"Warning: Infinite values found in {col}, replacing with 0\")\n","        reviews_df[col] = reviews_df[col].replace([np.inf, -np.inf], 0)\n","\n","    if reviews_df[col].abs().max() > 1e10:\n","        print(f\"Warning: Very large values in {col}, clipping to reasonable range\")\n","        reviews_df[col] = reviews_df[col].clip(-1e6, 1e6)\n","\n","# Fill any remaining NaN values\n","reviews_df = reviews_df.fillna(0)\n","\n","\n","# Ensure required columns exist for the next notebook\n","required_columns = [\n","    'text', 'cleaned_text', 'rating', 'time', 'user_id',\n","    'sentiment_score', 'review_day_of_week', 'review_hour_of_day', 'dominant_topic',\n","    'review_length_words', 'num_urls', 'num_emails', 'num_phone_numbers',\n","    'user_review_count', 'n_photos'\n","]\n","\n","missing_required = [col for col in required_columns if col not in reviews_df.columns]\n","if missing_required:\n","    print(f\"Warning: Missing required columns: {missing_required}\")\n","    for col in missing_required:\n","        reviews_df[col] = 0 if col != 'text' else ''\n","\n","print(f\"Final dataset shape: {reviews_df.shape}\")\n","print(f\"Memory usage: {reviews_df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n","\n","# Save the engineered features\n","try:\n","    reviews_df.to_pickle(OUTPUT_FILE)\n","    print(f\"Successfully saved DataFrame with {len(reviews_df)} rows to {OUTPUT_FILE}\") # Changed len(reviews) to len(reviews_df)\n","except Exception as e:\n","    print(f\"Error saving DataFrame: {e}\")"],"metadata":{"id":"od3vp7uAe9Oc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(reviews_df.info())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oxf87nlo0q2P","executionInfo":{"status":"ok","timestamp":1756580012127,"user_tz":-480,"elapsed":0,"user":{"displayName":"joshua teo","userId":"08987881000560814542"}},"outputId":"4bbea3a8-9434-4a65-8471-95e3d9a43649"},"execution_count":22,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 50000 entries, 0 to 49999\n","Data columns (total 70 columns):\n"," #   Column                      Non-Null Count  Dtype         \n","---  ------                      --------------  -----         \n"," 0   user_id                     50000 non-null  object        \n"," 1   name                        50000 non-null  object        \n"," 2   time                        50000 non-null  datetime64[ns]\n"," 3   rating                      50000 non-null  int64         \n"," 4   text                        50000 non-null  object        \n"," 5   pics                        50000 non-null  object        \n"," 6   resp                        50000 non-null  object        \n"," 7   gmap_id                     50000 non-null  object        \n"," 8   cleaned_text                50000 non-null  object        \n"," 9   sentiment_score             50000 non-null  float64       \n"," 10  review_day_of_week          50000 non-null  int32         \n"," 11  review_hour_of_day          50000 non-null  int32         \n"," 12  processed_text_for_topic    50000 non-null  object        \n"," 13  dominant_topic              50000 non-null  int64         \n"," 14  top_keywords                50000 non-null  object        \n"," 15  num_urls                    50000 non-null  int64         \n"," 16  num_emails                  50000 non-null  int64         \n"," 17  num_phone_numbers           50000 non-null  int64         \n"," 18  num_mentions                50000 non-null  int64         \n"," 19  num_hashtags                50000 non-null  int64         \n"," 20  num_irrelevant_words        50000 non-null  int64         \n"," 21  num_exclamations            50000 non-null  int64         \n"," 22  num_questions               50000 non-null  int64         \n"," 23  num_ellipsis                50000 non-null  int64         \n"," 24  num_total_punctuation       50000 non-null  int64         \n"," 25  review_length_words         50000 non-null  int64         \n"," 26  review_length_chars         50000 non-null  int64         \n"," 27  all_caps_word_count         50000 non-null  int64         \n"," 28  elongated_word_count        50000 non-null  int64         \n"," 29  caps_ratio                  50000 non-null  float64       \n"," 30  unique_word_ratio           50000 non-null  float64       \n"," 31  digit_ratio                 50000 non-null  float64       \n"," 32  punctuation_ratio           50000 non-null  float64       \n"," 33  num_sentiment_words_pos     50000 non-null  int64         \n"," 34  num_sentiment_words_neg     50000 non-null  int64         \n"," 35  num_emojis                  50000 non-null  int64         \n"," 36  sentiment_polarity_lex      50000 non-null  int64         \n"," 37  pos_words_per_100w          50000 non-null  float64       \n"," 38  neg_words_per_100w          50000 non-null  float64       \n"," 39  emojis_per_100w             50000 non-null  float64       \n"," 40  user_review_count           50000 non-null  int64         \n"," 41  num_same_day_reviews        50000 non-null  int64         \n"," 42  avg_review_rating           50000 non-null  float64       \n"," 43  user_max_7d_burst           50000 non-null  float64       \n"," 44  user_review_span_days       50000 non-null  int64         \n"," 45  user_burst_ratio            50000 non-null  float64       \n"," 46  user_place_diversity        50000 non-null  int64         \n"," 47  user_avg_length             50000 non-null  float64       \n"," 48  user_text_share             50000 non-null  float64       \n"," 49  user_response_rate          50000 non-null  float64       \n"," 50  n_photos                    50000 non-null  int64         \n"," 51  photo_url_length_mean       50000 non-null  float64       \n"," 52  photo_url_length_max        50000 non-null  float64       \n"," 53  photo_url_length_std        50000 non-null  float64       \n"," 54  photo_google_cdn_flag       50000 non-null  int64         \n"," 55  has_photo_and_no_text       50000 non-null  int64         \n"," 56  has_photo_and_short_text    50000 non-null  int64         \n"," 57  has_photo_and_low_rating    50000 non-null  int64         \n"," 58  has_photo_and_high_rating   50000 non-null  int64         \n"," 59  n_photos_user_z             50000 non-null  float64       \n"," 60  photo_url_param_count_mean  50000 non-null  float64       \n"," 61  photo_host_nunique          50000 non-null  float64       \n"," 62  photo_has_multi_hosts       50000 non-null  float64       \n"," 63  photo_has_thumbnail_param   50000 non-null  float64       \n"," 64  photo_url_is_duplicate_any  50000 non-null  float64       \n"," 65  user_photo_host_diversity   50000 non-null  float64       \n"," 66  user_avg_photo_url_len      50000 non-null  float64       \n"," 67  user_median_n_photos        50000 non-null  float64       \n"," 68  place_photo_host_diversity  50000 non-null  float64       \n"," 69  place_median_n_photos       50000 non-null  float64       \n","dtypes: datetime64[ns](1), float64(28), int32(2), int64(30), object(9)\n","memory usage: 26.3+ MB\n","None\n"]}]}]}